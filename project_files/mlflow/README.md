# MLflow Integration

MLflow integration for Customer Churn Prediction project. Simple experiment tracking and model management.

## üìÅ Files

### `mlflow_config.py`
- **Purpose**: Core MLflow configuration and setup
- **Features**:
  - SQLite backend for tracking
  - Local artifact storage
  - Simple experiment management
  - Model logging and loading
  - Best model retrieval

### `experiment_tracker.py`
- **Purpose**: Experiment tracking and model comparison
- **Features**:
  - Comprehensive metrics calculation
  - Confusion matrix generation
  - Model comparison utilities
  - Experiment results export
  - Best model identification

### `run_experiments.py`
- **Purpose**: Run MLflow experiments with different models
- **Features**:
  - Multiple model configurations
  - Automated experiment execution
  - Results comparison
  - Best model selection

### `start_mlflow_ui.ps1`
- **Purpose**: PowerShell script to start MLflow UI
- **Features**:
  - Easy UI startup
  - Port configuration
  - Error handling

## üîß Usage

### Setup MLflow

```powershell
# Install MLflow
uv add mlflow

# Start MLflow UI
.\project_files\mlflow\start_mlflow_ui.ps1
```

### Run Experiments

```powershell
# Run all experiments
uv run python project_files/mlflow/run_experiments.py

# Or from Python
python -c "
from project_files.mlflow import get_experiment_tracker
tracker = get_experiment_tracker()
# Use tracker methods
"
```

### Access MLflow UI

1. Start the UI: `.\project_files\mlflow\start_mlflow_ui.ps1`
2. Open browser: http://localhost:5000
3. View experiments, runs, and models

## üéØ Features

### Experiment Tracking
- ‚úÖ **Automatic run creation** with timestamps
- ‚úÖ **Parameter logging** for all model configurations
- ‚úÖ **Metric tracking** (accuracy, precision, recall, F1, ROC AUC)
- ‚úÖ **Model artifacts** storage and versioning
- ‚úÖ **Confusion matrix** visualization

### Model Management
- ‚úÖ **Model logging** with MLflow format
- ‚úÖ **Best model retrieval** based on metrics
- ‚úÖ **Model loading** from MLflow runs
- ‚úÖ **Model comparison** across experiments

### Visualization
- ‚úÖ **MLflow UI** for experiment browsing
- ‚úÖ **Confusion matrices** for each model
- ‚úÖ **Metric comparison** tables
- ‚úÖ **Experiment results** export to CSV

## üìä Available Metrics

### Classification Metrics
- **Accuracy**: Overall prediction accuracy
- **Precision**: Precision for each class
- **Recall**: Recall for each class
- **F1 Score**: Harmonic mean of precision and recall
- **ROC AUC**: Area under ROC curve
- **Log Loss**: Logarithmic loss

### Per-Class Metrics
- **Class 0 (Not Churned)**: Precision, recall, F1
- **Class 1 (Churned)**: Precision, recall, F1

## üî¨ Experiment Types

### Default Experiments
1. **LogisticRegression**: Linear model with regularization
2. **RandomForest**: Ensemble of decision trees
3. **GradientBoosting**: Sequential boosting model

### Custom Experiments
```python
from project_files.mlflow import get_experiment_tracker
from sklearn.svm import SVC

tracker = get_experiment_tracker()

# Custom experiment
svm_model = SVC(probability=True, random_state=42)
run_id, metrics = tracker.log_model_experiment(
    model=svm_model,
    model_name="SVM",
    X_train=X_train,
    X_test=X_test,
    y_train=y_train,
    y_test=y_test,
    params={'C': 1.0, 'kernel': 'rbf'}
)
```

## üìà Model Comparison

### Automatic Comparison
```python
# Compare all models
comparison_df = tracker.compare_models(results)
print(comparison_df[['model_name', 'f1_score', 'accuracy', 'roc_auc']])
```

### Best Model Selection
```python
# Get best model info
best_info = tracker.get_best_model_info(metric_name="f1_score")
print(f"Best model: {best_info['model_name']}")
print(f"F1 Score: {best_info['metrics']['f1_score']}")
```

## üóÇÔ∏è File Structure

```
project_files/mlflow/
‚îú‚îÄ‚îÄ __init__.py                    # Package initialization
‚îú‚îÄ‚îÄ mlflow_config.py               # Core MLflow configuration
‚îú‚îÄ‚îÄ experiment_tracker.py           # Experiment tracking utilities
‚îú‚îÄ‚îÄ run_experiments.py             # Experiment runner script
‚îú‚îÄ‚îÄ start_mlflow_ui.ps1           # MLflow UI launcher
‚îú‚îÄ‚îÄ README.md                      # This documentation
‚îú‚îÄ‚îÄ mlflow.db                      # SQLite tracking database
‚îú‚îÄ‚îÄ artifacts/                     # Model artifacts storage
‚îî‚îÄ‚îÄ confusion_matrix_*.png         # Generated confusion matrices
```

## üöÄ Quick Start

### 1. Install Dependencies
```powershell
uv add mlflow matplotlib seaborn
```

### 2. Run Experiments
```powershell
uv run python project_files/mlflow/run_experiments.py
```

### 3. Start MLflow UI
```powershell
.\project_files\mlflow\start_mlflow_ui.ps1
```

### 4. View Results
- Open http://localhost:5000
- Browse experiments and runs
- Compare model performance
- Download artifacts

## üìã Best Practices

1. **Always log parameters** for reproducibility
2. **Use consistent metric names** across experiments
3. **Export results** for external analysis
4. **Tag important runs** for easy identification
5. **Regular cleanup** of old artifacts

## üîÑ Integration

### With Existing Pipeline
```python
# In customer_churn_prediction.py
from project_files.mlflow import log_experiment_results

# After model training
log_experiment_results(
    params=model_params,
    metrics=model_metrics,
    model=trained_model
)
```

### With FastAPI
```python
# Load best model for API
from project_files.mlflow import load_best_model

best_model = load_best_model()
# Use in API predictions
```

## üìñ Documentation

- **MLflow Documentation**: https://mlflow.org/docs/
- **MLflow Python API**: https://mlflow.org/docs/latest/python_api/
- **MLflow UI Guide**: https://mlflow.org/docs/latest/tracking.html#tracking-ui

## üõ†Ô∏è Customization

### Adding New Metrics
```python
# In experiment_tracker.py
def _calculate_metrics(self, y_true, y_pred, y_pred_proba):
    metrics = super()._calculate_metrics(y_true, y_pred, y_pred_proba)
    
    # Add custom metrics
    from sklearn.metrics import balanced_accuracy_score
    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)
    
    return metrics
```

### Custom Model Logging
```python
# Log custom model types
def log_custom_model(self, model, model_name, custom_loader=None):
    if custom_loader:
        mlflow.pyfunc.log_model(model_name, python_model=custom_loader)
    else:
        mlflow.sklearn.log_model(model, model_name)
``` 